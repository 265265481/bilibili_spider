{
 "cells": [
  {
   "cell_type": "code",
   "id": "d08cc89f-48d7-4ef3-bdfb-d8b25c4346d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T02:42:32.934830Z",
     "start_time": "2025-08-05T02:42:15.530871Z"
    }
   },
   "source": [
    "# 根据用户ID获得指定时间段的视频信息\n",
    "import requests\n",
    "import hashlib\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import jsonpath\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from urllib.parse import urlencode\n",
    "from openpyxl import Workbook, load_workbook\n",
    "import urllib.parse\n",
    "import traceback\n",
    "import random\n",
    "import threading\n",
    "\n",
    "# --- 配置区域 ---\n",
    "# 请务必修改这里的配置\n",
    "CONFIG = {\n",
    "    # 确保这个Excel文件路径是正确的\n",
    "    # 这个Excel文件应该包含要获取UP主MID号，MID号默认在第一列（索引为0）\n",
    "    \"excel_path\": r\"F:\\Code\\爬虫\\UP主ID.xlsx\",  # <--- 请替换为你的Excel文件路径\n",
    "\n",
    "    # B站登录Cookie字符串池：重要！用于保持会话稳定性和反爬。请提供至少一个有效Cookie。\n",
    "    # 获取方法：登录B站 -> F12开发者工具 -> Network (网络) -> 刷新页面 -> 找到任意请求 -> Headers (请求头) -> Request Headers (请求头) 中找到 'Cookie' 字段，复制其完整内容。\n",
    "    \"cookie_strings_pool\": [\n",
    "        \"buvid3=384BBEAA-858B-A2E9-5B83-849BC33FA9C630622infoc; b_nut=1741237430; _uuid=4BBF9EA4-1FE10-3ECA-B936-D37E18A4104B730789infoc; enable_web_push=DISABLE; buvid4=3EC2F346-3DDE-A9E7-79EC-DD96376E9C5931240-025030605-UEW7z%2Frhc9FUd5uaNwO%2FDQ%3D%3D; buvid_fp=0f85c81c4fa8403529178b71f34e4055; rpdid=0zbfVJ1vzQ|7Qh8tt0I|2lF|3w1U6854; enable_feed_channel=ENABLE; theme-tip-show=SHOWED; theme-avatar-tip-show=SHOWED; theme-switch-show=SHOWED; bp_t_offset_355987571=1087855640585437184; header_theme_version=OPEN; bp_t_offset_435641086=1092808085770076160; b_lsid=C5FCC3FF_1987818959F; bsource=search_bing; bili_ticket=eyJhbGciOiJIUzI1NiIsImtpZCI6InMwMyIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NTQ2MjA3MzIsImlhdCI6MTc1NDM2MTQ3MiwicGx0IjotMX0.HthfuyxINbFh1eFX5wF4oQ4exeKxXVeadrliEFJQJs4; bili_ticket_expires=1754620672; bp_t_offset_3493111795812748=1097463538261164032; CURRENT_FNVAL=2000; csrf_state=a2a7469d11ecb38b0cf9dc6600a52207; SESSDATA=720472af%2C1769913559%2Ccf013%2A82CjDtXnMmG5j4aZlVtvGckgP6ZiLzw9NpGJNtXFdWCasXC0Gi-LK17RfYB5OJH4EGhzESVmlPaEVUQW1rc0pLc0VLdDVtc2p2Tlp6ZVR5c25EQXZnc1NjRVFOcU0yOGtTQV9BVldzdUFvbGtDODZEWjZ6VC1OUW5uMl9ZQ1Z3a2Y5UU93VWE1UHFBIIEC; bili_jct=71f8500b3f14f993f7c50a54de0638cd; DedeUserID=3546919328549020; DedeUserID__ckMd5=34f98e6ac83665d9; sid=5awtpcjx; bp_t_offset_3546919328549020=1097463632750444544; home_feed_column=4; browser_resolution=763-834\"# 可以添加更多Cookie字符串\n",
    "    ],\n",
    "    \"cookie_rotate_interval_seconds\": 900,  # Cookie轮换间隔，单位秒 (900秒 = 15分钟)\n",
    "\n",
    "    # 视频时间筛选范围 2023年7月1日至2024年1月31日\n",
    "    \"start_date\": \"2023-07-01\",  # 开始日期，格式：YYYY-MM-DD\n",
    "    \"end_date\": \"2024-01-31\",  # 结束日期，格式：YYYY-MM-DD\n",
    "\n",
    "    # 批量处理UP主的范围（从Excel中读取UP主列表的索引）\n",
    "    \"start_index\": 750,  # 开始位置（从0开始）\n",
    "    \"end_index\": None,  # 结束位置（None表示到文件末尾）\n",
    "\n",
    "    # 每个UP主最大爬取页数：B站视频列表每页40个，可以根据UP主视频数量和需要设定。\n",
    "    \"max_pages_per_up\": 20,\n",
    "}\n",
    "\n",
    "def sanitize_filename(name):\n",
    "    \"\"\"移除文件名中的非法字符\"\"\"\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"\", name)\n",
    "\n",
    "def read_up_list_from_excel(file_path, start_index=0, end_index=None):\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"❌ 错误: '{file_path}' 文件不存在!\")\n",
    "            return []\n",
    "        print(f\"📊 正在读取Excel文件: {file_path}\")\n",
    "        df = pd.read_excel(file_path)\n",
    "\n",
    "        up_mids = df.iloc[:, 0].tolist()  # 获取第一列数据\n",
    "\n",
    "        up_mids = [str(mid).strip() for mid in up_mids if pd.notna(mid) and str(mid).strip()]\n",
    "\n",
    "        # 移除可能的表头（如果第一行是字符串而不是MID）\n",
    "        if up_mids and not up_mids[0].isdigit():  \n",
    "            up_mids = up_mids[1:]\n",
    "\n",
    "        total_count = len(up_mids)\n",
    "        print(f\"✅ 成功读取到 {total_count} 个UP主ID。\")\n",
    "\n",
    "        # 应用索引范围\n",
    "        if end_index is None:\n",
    "            end_index = total_count\n",
    "        else:\n",
    "            end_index = min(end_index, total_count)\n",
    "        \n",
    "        start_index = max(0, start_index)  # 确保start_index不为负\n",
    "\n",
    "        if start_index >= total_count and total_count > 0:  # 如果文件非空但起始索引超出\n",
    "            print(f\"⚠️ 指定的开始索引 {start_index} 超出Excel中UP主ID的总数 {total_count}。\")\n",
    "            return []\n",
    "        elif total_count == 0:  # 文件为空或无有效数据\n",
    "            print(\"⚠️ Excel文件中没有找到有效的UP主ID。\")\n",
    "            return []\n",
    "\n",
    "\n",
    "        selected_up_mids = up_mids[start_index:end_index]\n",
    "\n",
    "        print(f\"🎯 选择处理范围: [{start_index}:{end_index}]，共 {len(selected_up_mids)} 个UP主。\")\n",
    "        return selected_up_mids\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 读取Excel文件失败: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def write_results_to_excel(df: pd.DataFrame, file_path: str):\n",
    "    try:\n",
    "        df.to_excel(file_path, index=False)\n",
    "        print(f\"💾 数据已保存到 '{file_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 写入Excel出错: {str(e)}\")\n",
    "\n",
    "class BilibiliCrawler:\n",
    "    DEFAULT_REQUEST_TIMEOUT = 60  # 请求超时时间\n",
    "    DEFAULT_RETRY_COUNT = 5  # 重试次数\n",
    "    DEFAULT_BACKOFF_FACTOR = 9  # 重试间隔因子 (1s, 3s, 9s, ...)\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        self.cookie_strings_pool = config[\"cookie_strings_pool\"]\n",
    "        self.cookie_rotate_interval_seconds = config[\"cookie_rotate_interval_seconds\"]\n",
    "        self.current_cookie_index = 0\n",
    "        self.last_cookie_change_time = time.time()\n",
    "        self.cookies = {}  # requests library cookie jar\n",
    "        \n",
    "        # User-Agent池\n",
    "        self.user_agent_pool = [\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0',\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:128.0) Gecko/20100101 Firefox/128.0',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/126.0.2592.56',\n",
    "            'Mozilla/5.0 (Linux; Android 14) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.6478.140 Mobile Safari/537.36',\n",
    "            'Mozilla/5.0 (iPhone; CPU iPhone OS 17_5_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.5 Mobile/15E148 Safari/604.1',\n",
    "        ]\n",
    "\n",
    "        self.headers = {\n",
    "            'Accept': 'application/json, text/plain, */*',\n",
    "            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Referer': 'https://space.bilibili.com/',\n",
    "            'Origin': 'https://space.bilibili.com',\n",
    "            'Sec-Fetch-Dest': 'empty',\n",
    "            'Sec-Fetch-Mode': 'cors',\n",
    "            'Sec-Fetch-Site': 'same-site',\n",
    "            'Cookie': ''  # 初始为空，由_set_current_cookie设置\n",
    "        }\n",
    "        \n",
    "        self.wbi_keys = None  # 用于视频列表API的WBI密钥\n",
    "        self.mixinKeyEncTab = [  # WBI签名用到的固定数组\n",
    "            46, 47, 18, 2, 53, 8, 23, 32, 15, 50, 10, 31, 58, 3, 45, 35, 27, 43, 5, 49,\n",
    "            33, 9, 42, 19, 29, 28, 14, 39, 12, 38, 41, 13, 37, 48, 7, 16, 24, 55, 40,\n",
    "            61, 26, 17, 0, 1, 60, 51, 30, 4, 22, 25, 54, 21, 56, 59, 6, 63, 57, 62, 11,\n",
    "            36, 20, 34, 44, 52\n",
    "        ]\n",
    "\n",
    "        # 初始化时设置第一个 Cookie\n",
    "        self._set_current_cookie()  \n",
    "\n",
    "        # 统计数据\n",
    "        self.batch_stats = {\n",
    "            'total_processed': 0,\n",
    "            'success_count': 0,\n",
    "            'failed_count': 0,\n",
    "            'total_videos': 0,\n",
    "        }\n",
    "\n",
    "        # 时间筛选范围\n",
    "        self.start_timestamp = int(datetime.datetime.strptime(config[\"start_date\"], \"%Y-%m-%d\").timestamp())\n",
    "        self.end_timestamp = int(datetime.datetime.strptime(config[\"end_date\"] + \" 23:59:59\", \"%Y-%m-%d %H:%M:%S\").timestamp())\n",
    "        self.start_date = config[\"start_date\"]\n",
    "        self.end_date = config[\"end_date\"]\n",
    "\n",
    "    def _parse_cookies(self, cookie_string):\n",
    "        \"\"\"解析cookie字符串为字典\"\"\"\n",
    "        cookies = {}\n",
    "        for item in cookie_string.split(';'):\n",
    "            if '=' in item:\n",
    "                key, value = item.strip().split('=', 1)\n",
    "                cookies[key] = value\n",
    "        return cookies\n",
    "\n",
    "    def _set_current_cookie(self):\n",
    "        \"\"\"设置当前使用的 Cookie\"\"\"\n",
    "        if not self.cookie_strings_pool:\n",
    "            print(\"❌ Cookie池为空，将无法进行Cookie轮换。\")\n",
    "            self.cookies = {}  # requests library cookie jar\n",
    "            self.headers['Cookie'] = \"\"  # http header cookie string\n",
    "            return\n",
    "\n",
    "        cookie_string = self.cookie_strings_pool[self.current_cookie_index]\n",
    "        self.cookies = self._parse_cookies(cookie_string)\n",
    "        self.headers['Cookie'] = cookie_string  \n",
    "        self.last_cookie_change_time = time.time()\n",
    "        print(f\"✅ Cookie已切换至池中索引 {self.current_cookie_index} 的Cookie。\")\n",
    "\n",
    "    def _rotate_cookie_if_needed(self):\n",
    "        \"\"\"根据时间间隔轮换Cookie\"\"\"\n",
    "        if not self.cookie_strings_pool or len(self.cookie_strings_pool) <= 1:\n",
    "            return\n",
    "\n",
    "        if (time.time() - self.last_cookie_change_time) >= self.cookie_rotate_interval_seconds:\n",
    "            self.current_cookie_index = (self.current_cookie_index + 1) % len(self.cookie_strings_pool)\n",
    "            self._set_current_cookie()\n",
    "            print(f\"🔄 Cookie 达到 {self.cookie_rotate_interval_seconds} 秒轮换周期，已切换到新 Cookie (索引: {self.current_cookie_index})。\")\n",
    "\n",
    "    def _make_request(self, url, method=\"GET\", params=None, data=None, json_data=None, extra_headers=None):\n",
    "        self._rotate_cookie_if_needed()  # 每次请求前检查并轮换Cookie\n",
    "\n",
    "        current_attempt = 0\n",
    "        request_headers = self.headers.copy()  # headers已包含User-Agent和当前Cookie\n",
    "\n",
    "        request_headers['User-Agent'] = random.choice(self.user_agent_pool)  # 随机选择User-Agent\n",
    "\n",
    "        if extra_headers:\n",
    "            request_headers.update(extra_headers)\n",
    "\n",
    "        while current_attempt < self.DEFAULT_RETRY_COUNT:  # 使用类属性\n",
    "            try:\n",
    "                if method.upper() == \"GET\":\n",
    "                    response = requests.get(\n",
    "                        url,\n",
    "                        headers=request_headers,\n",
    "                        cookies=self.cookies,  # 使用self.cookies对象，它会在_set_current_cookie中更新\n",
    "                        params=params,\n",
    "                        timeout=self.DEFAULT_REQUEST_TIMEOUT  # 使用类属性\n",
    "                    )\n",
    "                elif method.upper() == \"POST\":\n",
    "                    response = requests.post(\n",
    "                        url,\n",
    "                        headers=request_headers,\n",
    "                        cookies=self.cookies,  # 使用self.cookies对象\n",
    "                        params=params,\n",
    "                        data=data,\n",
    "                        json=json_data,\n",
    "                        timeout=self.DEFAULT_REQUEST_TIMEOUT  # 使用类属性\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(f\"不支持的HTTP方法: {method}\")\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    return response\n",
    "                elif response.status_code in [403, 412, 429, 500, 502, 503, 504]:\n",
    "                    print(f\"HTTP错误: {response.status_code}. 尝试重试...\")\n",
    "                    time.sleep(self.DEFAULT_BACKOFF_FACTOR ** current_attempt)  # 使用类属性\n",
    "                else:\n",
    "                    print(f\"非200 HTTP状态码: {response.status_code}.\")\n",
    "                    return response\n",
    "\n",
    "            except requests.exceptions.Timeout:\n",
    "                print(\"请求超时。尝试重试...\")\n",
    "                time.sleep(self.DEFAULT_BACKOFF_FACTOR ** current_attempt)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"请求异常: {e}. 尝试重试...\")\n",
    "                time.sleep(self.DEFAULT_BACKOFF_FACTOR ** current_attempt)\n",
    "            except Exception as e:\n",
    "                print(f\"未知错误: {e}.\")\n",
    "                return None\n",
    "\n",
    "            current_attempt += 1\n",
    "\n",
    "        print(f\"❌ 达到最大重试次数，请求失败: {url}\")\n",
    "        return None\n",
    "\n",
    "    def test_cookie_validity(self):\n",
    "        \"\"\"测试Cookie池中当前Cookie的有效性\"\"\"\n",
    "        test_url = \"https://api.bilibili.com/x/web-interface/nav\"\n",
    "        \n",
    "        # 使用当前加载到self.cookies和self.headers['Cookie']的Cookie进行测试\n",
    "        response = self._make_request(test_url)\n",
    "\n",
    "        if response and response.status_code == 200:\n",
    "            try:\n",
    "                data = response.json()\n",
    "                if data.get('code') == 0:\n",
    "                    user_info = data.get('data', {})\n",
    "                    return True, {\n",
    "                        'username': user_info.get('uname', '未知'),\n",
    "                        'uid': user_info.get('mid', '未知'),\n",
    "                        'level': user_info.get('level_info', {}).get('current_level', 0),\n",
    "                        'coins': user_info.get('money', 0),\n",
    "                        'vip_status': user_info.get('vipStatus', 0)\n",
    "                    }\n",
    "                else:\n",
    "                    return False, f\"API返回错误: {data.get('message', '未知错误')} (Code: {data.get('code')})\"\n",
    "            except json.JSONDecodeError:\n",
    "                return False, \"无法解析Cookie测试API的响应为JSON。\"\n",
    "        else:\n",
    "            status_code = response.status_code if response else 'N/A'\n",
    "            return False, f\"HTTP请求失败或无响应，状态码: {status_code}\"\n",
    "\n",
    "    # --- B站WBI签名算法 (用于视频列表和详情) ---\n",
    "    def get_mixin_key(self, orig: str) -> str:\n",
    "        return ''.join([orig[i] for i in self.mixinKeyEncTab])[:32]\n",
    "\n",
    "    def enc_wbi(self, params: dict, img_key: str, sub_key: str) -> dict:\n",
    "        mixin_key = self.get_mixin_key(img_key + sub_key)\n",
    "        curr_time = round(time.time())\n",
    "        params['wts'] = curr_time\n",
    "\n",
    "        sorted_params = sorted(params.items())\n",
    "        query = urlencode(sorted_params)\n",
    "\n",
    "        wbi_sign = hashlib.md5((query + mixin_key).encode()).hexdigest()\n",
    "        params['w_rid'] = wbi_sign\n",
    "        return params\n",
    "\n",
    "    def get_wbi_keys(self):\n",
    "        # 优化：每次只在需要时才获取 WBI 密钥，并缓存\n",
    "        if hasattr(self, '_cached_wbi_keys') and self._cached_wbi_keys:\n",
    "            return self._cached_wbi_keys\n",
    "\n",
    "        response = self._make_request(\n",
    "            'https://api.bilibili.com/x/web-interface/nav'\n",
    "        )\n",
    "\n",
    "        if response and response.status_code == 200:\n",
    "            try:\n",
    "                json_content = response.json()\n",
    "                if json_content.get('code') == 0:\n",
    "                    wbi_img = json_content['data']['wbi_img']\n",
    "                    img_url: str = wbi_img['img_url']\n",
    "                    sub_url: str = wbi_img['sub_url']\n",
    "                    img_key = img_url.rsplit('/', 1)[1].split('.')[0]\n",
    "                    sub_key = sub_url.rsplit('/', 1)[1].split('.')[0]\n",
    "                    self._cached_wbi_keys = (img_key, sub_key)  # 缓存密钥\n",
    "                    return img_key, sub_key\n",
    "                else:\n",
    "                    print(f\"获取WBI密钥失败（API错误）: {json_content.get('message')} (Code: {json_content.get('code')})\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"获取WBI密钥响应JSON解析失败。\")\n",
    "        else:\n",
    "            status_code = response.status_code if response else 'N/A'\n",
    "            print(f\"获取WBI密钥HTTP请求失败，状态码: {status_code}\")\n",
    "\n",
    "        return None, None\n",
    "\n",
    "    def generate_api_url(self, mid, page=1, ps=40):\n",
    "        if not self.wbi_keys:\n",
    "            img_key, sub_key = self.get_wbi_keys()\n",
    "            if not img_key or not sub_key:\n",
    "                print(\"❌ 无法生成视频列表API的wbi密钥\")\n",
    "                return None\n",
    "            self.wbi_keys = (img_key, sub_key)\n",
    "        else:\n",
    "            img_key, sub_key = self.wbi_keys\n",
    "\n",
    "        params = {\n",
    "            'mid': str(mid),\n",
    "            'ps': str(ps),\n",
    "            'tid': '0',\n",
    "            'pn': str(page),\n",
    "            'keyword': '',\n",
    "            'order': 'pubdate',\n",
    "            'platform': 'web',\n",
    "            'web_location': '333.1387',\n",
    "            'order_avoided': 'true'\n",
    "        }\n",
    "\n",
    "        signed_params = self.enc_wbi(params, img_key, sub_key)\n",
    "        base_url = \"https://api.bilibili.com/x/space/wbi/arc/search\"\n",
    "        url = f\"{base_url}?{urlencode(signed_params)}\"\n",
    "\n",
    "        return url\n",
    "\n",
    "    # --- 获取UP主信息 ---\n",
    "    def get_up_info(self, mid):\n",
    "        # 方案1: 尝试获取详细信息 (需要wbi)\n",
    "        url = f\"https://api.bilibili.com/x/space/wbi/acc/info?mid={mid}\"\n",
    "        response = self._make_request(url)\n",
    "\n",
    "        if response and response.status_code == 200:\n",
    "            try:\n",
    "                data = response.json()\n",
    "                if data.get('code') == 0:\n",
    "                    return data['data']\n",
    "                else:\n",
    "                    print(f\"详细信息获取失败: {data.get('message', '未知错误')} (Code: {data.get('code')})\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"详细信息响应JSON解析失败。\")\n",
    "\n",
    "        # 方案2: 尝试获取基础信息 (可能不需要wbi)\n",
    "        print(\"🔄 尝试获取基础UP主信息...\")\n",
    "        basic_url = f\"https://api.bilibili.com/x/space/acc/info?mid={mid}\"\n",
    "        response = self._make_request(basic_url)\n",
    "\n",
    "        if response and response.status_code == 200:\n",
    "            try:\n",
    "                data = response.json()\n",
    "                if data.get('code') == 0:\n",
    "                    return data['data']\n",
    "                else:\n",
    "                    print(f\"基础信息获取失败: {data.get('message', '未知错误')} (Code: {data.get('code')})\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"基础信息响应JSON解析失败。\")\n",
    "\n",
    "        # 方案3: 从视频页面获取信息 (如果前两者都失败)\n",
    "        try:\n",
    "            print(\"🔄 尝试从视频页面获取UP主信息...\")\n",
    "            first_url = self.generate_api_url(mid, 1)\n",
    "            if first_url:\n",
    "                response = self._make_request(first_url)\n",
    "                if response and response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    if data.get('code') == 0:\n",
    "                        vlist = data.get('data', {}).get('list', {}).get('vlist', [])\n",
    "                        if vlist:\n",
    "                            first_video = vlist[0]\n",
    "                            return {\n",
    "                                'name': first_video.get('author', f'UP主{mid}'),\n",
    "                                'mid': mid,\n",
    "                                'follower': 0,\n",
    "                                'video': len(vlist)\n",
    "                            }\n",
    "            print(\"❌ 从视频页面获取UP主信息失败。\")\n",
    "        except Exception as e:\n",
    "            print(f\"从视频页面获取信息时发生异常: {e}\")\n",
    "\n",
    "        return None\n",
    "\n",
    "    # --- 获取视频详细信息（包含播放、点赞、收藏等） ---\n",
    "    def get_video_detail(self, aid):\n",
    "        if not aid:\n",
    "            return None\n",
    "\n",
    "        url = \"https://api.bilibili.com/x/web-interface/wbi/view/detail\"\n",
    "        if not self.wbi_keys:\n",
    "            self.get_wbi_keys()\n",
    "        if not self.wbi_keys:\n",
    "            print(\"❌ 无法获取视频详情API的wbi密钥，跳过详情获取。\")\n",
    "            return None\n",
    "\n",
    "        img_key, sub_key = self.wbi_keys\n",
    "        params = {\"aid\": aid}\n",
    "        signed_params = self.enc_wbi(params, img_key, sub_key)\n",
    "\n",
    "        response = self._make_request(\n",
    "            url,\n",
    "            params=signed_params\n",
    "        )\n",
    "\n",
    "        if not response or response.status_code != 200:\n",
    "            print(f\"获取视频 {aid} 详情失败，HTTP状态码: {response.status_code if response else 'N/A'}\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            json_data = response.json()\n",
    "            if json_data.get('code') != 0:\n",
    "                print(f\"详情API返回错误: {json_data.get('message', '未知错误')} (Code: {json_data.get('code')})\")\n",
    "                return None\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"详情JSON解析失败\")\n",
    "            return None\n",
    "\n",
    "        view = jsonpath.jsonpath(json_data, '$..View.stat')\n",
    "        tags = jsonpath.jsonpath(json_data, '$..Tags')\n",
    "        Cards = jsonpath.jsonpath(json_data, '$..Card..card')\n",
    "        duration_seconds_list = jsonpath.jsonpath(json_data, '$..View.duration')\n",
    "        duration_seconds = duration_seconds_list[0] if duration_seconds_list else 'N/A'\n",
    "\n",
    "        pages_list = jsonpath.jsonpath(json_data, '$..View.pages')\n",
    "        pages_count = len(pages_list[0]) if pages_list and pages_list[0] else 0\n",
    "\n",
    "        copyright_val = jsonpath.jsonpath(json_data, '$..View.copyright')\n",
    "        copyright = '原创' if copyright_val and copyright_val[0] == 1 else '转载'\n",
    "\n",
    "        video_state_val = jsonpath.jsonpath(json_data, '$..View.state')\n",
    "        video_state = video_state_val[0] if video_state_val else 'N/A'\n",
    "\n",
    "        related_list = jsonpath.jsonpath(json_data, '$..Related')\n",
    "        related_count = len(related_list[0]) if related_list and related_list[0] else 0\n",
    "\n",
    "        mid = name = fans = sign = Official = level_info = 'N/A'\n",
    "        sex = vip_type = vip_status = nameplate = 'N/A'\n",
    "\n",
    "        if Cards:\n",
    "            card_data = Cards[0]\n",
    "            mid = card_data.get('mid', 'N/A')\n",
    "            name = card_data.get('name', 'N/A')\n",
    "            fans = card_data.get('fans', 'N/A')\n",
    "            sign = card_data.get('sign', 'N/A')\n",
    "            sex = card_data.get('sex', 'N/A')\n",
    "\n",
    "            if card_data.get('Official'):\n",
    "                Official = card_data.get('Official').get('title', '')\n",
    "            if card_data.get('level_info'):\n",
    "                level_info = f\"{Official}/{card_data.get('level_info').get('current_level')}\"\n",
    "\n",
    "            vip_data = card_data.get('vip', {})\n",
    "            vip_type_val = vip_data.get('type')\n",
    "            if vip_type_val == 1:\n",
    "                vip_type = '月度大会员'\n",
    "            elif vip_type_val == 2:\n",
    "                vip_type = '年度及以上大会员'\n",
    "            else:\n",
    "                vip_type = '无'\n",
    "            vip_status = '有效' if vip_data.get('status') == 1 else '无效'\n",
    "\n",
    "            nameplate_data = card_data.get('nameplate', {})\n",
    "            nameplate = nameplate_data.get('name', '无')\n",
    "\n",
    "        region = 'N/A'\n",
    "        if tags and tags[0]:\n",
    "            region_list = [tag.get('tag_name') for tag in tags[0]]\n",
    "            region = '、'.join(region_list)\n",
    "\n",
    "        like = collect = coin = share = 'N/A'\n",
    "        if view:\n",
    "            view_data = view[0]\n",
    "            like = view_data.get('like', 'N/A')\n",
    "            collect = view_data.get('favorite', 'N/A')\n",
    "            coin = view_data.get('coin', 'N/A')\n",
    "            share = view_data.get('share', 'N/A')\n",
    "\n",
    "        return (like, collect, coin, share, region, mid, name, fans, sign, level_info, duration_seconds,\n",
    "                pages_count, copyright, video_state, related_count, sex, vip_type, vip_status, nameplate)\n",
    "\n",
    "    # --- 时间筛选相关功能 ---\n",
    "    def is_video_in_time_range(self, created_timestamp):\n",
    "        return self.start_timestamp <= created_timestamp <= self.end_timestamp\n",
    "\n",
    "    def format_timestamp(self, timestamp):\n",
    "        return datetime.datetime.fromtimestamp(timestamp).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    def timestamp_to_datetime(self, timestamp):\n",
    "        dt_object = datetime.datetime.fromtimestamp(timestamp)\n",
    "        return dt_object.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # --- 视频列表爬取逻辑 ---\n",
    "    def get_filtered_videos_from_url(self, url):\n",
    "        response = self._make_request(url)\n",
    "\n",
    "        if not response or response.status_code != 200:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'message': f\"❌ HTTP错误或无响应: {response.status_code if response else 'N/A'}\"\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "            if data.get('code') == 0:\n",
    "                vlist = data.get('data', {}).get('list', {}).get('vlist', [])\n",
    "                page_info = data.get('data', {}).get('page', {})\n",
    "\n",
    "                filtered_videos = []\n",
    "                all_videos_info = []\n",
    "\n",
    "                for video in vlist:\n",
    "                    created_time = video.get('created', 0)\n",
    "                    video_info = {\n",
    "                        'created_timestamp': created_time,\n",
    "                        'created_date': self.format_timestamp(created_time),\n",
    "                        'in_range': self.is_video_in_time_range(created_time),\n",
    "                        'video_data': video\n",
    "                    }\n",
    "                    all_videos_info.append(video_info)\n",
    "\n",
    "                    if self.is_video_in_time_range(created_time):\n",
    "                        filtered_videos.append(video)\n",
    "\n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'all_videos': vlist,\n",
    "                    'all_videos_info': all_videos_info,\n",
    "                    'filtered_videos': filtered_videos,\n",
    "                    'total_count': page_info.get('count', 0),\n",
    "                    'current_page': page_info.get('pn', 1),\n",
    "                    'message': f\"✅ 页面获取{len(vlist)}个视频，筛选后{len(filtered_videos)}个\"\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'message': f\"❌ API返回错误: {data.get('message', '未知错误')} (Code: {data.get('code')})\"\n",
    "                }\n",
    "        except json.JSONDecodeError:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'message': \"❌ JSON解析失败\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'message': f\"❌ 请求处理异常: {str(e)}\"\n",
    "            }\n",
    "\n",
    "    def generate_api_urls(self, mid, max_pages=50):\n",
    "        print(f\"🎯 为UP主 {mid} 生成API URL（时间范围: {self.start_date} 到 {self.end_date}）...\")\n",
    "\n",
    "        first_url = self.generate_api_url(mid, 1)\n",
    "        if not first_url:\n",
    "            print(\"❌ 无法生成第一页URL\")\n",
    "            return {}\n",
    "\n",
    "        print(f\"📋 测试第一页URL...\")\n",
    "        result = self.get_filtered_videos_from_url(first_url)\n",
    "        if not result['success']:\n",
    "            print(f\"❌ 第1页测试失败: {result['message']}\")\n",
    "            print(\"🔄 尝试重新获取wbi密钥...\")\n",
    "            self.wbi_keys = None\n",
    "            first_url = self.generate_api_url(mid, 1)\n",
    "            if first_url:\n",
    "                result = self.get_filtered_videos_from_url(first_url)\n",
    "                if not result['success']:\n",
    "                    print(f\"❌ 重试后仍然失败: {result['message']}\")\n",
    "                    return {}\n",
    "            else:\n",
    "                return {}\n",
    "\n",
    "        total_videos = result.get('total_count', 0)\n",
    "        filtered_count = len(result.get('filtered_videos', []))\n",
    "        videos_per_page = 40\n",
    "\n",
    "        print(f\"📊 第一页统计: 总视频 {len(result.get('all_videos', []))} 个，筛选后 {filtered_count} 个\")\n",
    "\n",
    "        if total_videos > 0:\n",
    "            actual_pages = (total_videos + videos_per_page - 1) // videos_per_page\n",
    "            print(f\"📊 UP主统计: 总视频数 {total_videos}, 预计页数 {actual_pages}\")\n",
    "        else:\n",
    "            print(\"📊 无法获取视频总数，默认生成前几页\")\n",
    "            actual_pages = max_pages\n",
    "\n",
    "        pages_to_generate = min(actual_pages, max_pages)\n",
    "\n",
    "        api_urls = {}\n",
    "        api_urls[1] = first_url\n",
    "\n",
    "        for page in range(2, pages_to_generate + 1):\n",
    "            url = self.generate_api_url(mid, page)\n",
    "            if url:\n",
    "                api_urls[page] = url\n",
    "                print(f\"✅ 生成第{page}页API URL\")\n",
    "                time.sleep(7)\n",
    "            else:\n",
    "                print(f\"❌ 生成第{page}页URL失败\")\n",
    "                break\n",
    "\n",
    "        return api_urls\n",
    "\n",
    "    def crawl_videos_from_page_with_time_filter(self, page):\n",
    "        if page not in self.api_urls:\n",
    "            print(f\"第 {page} 页的URL不存在\")\n",
    "            return [], False\n",
    "\n",
    "        url = self.api_urls[page]\n",
    "\n",
    "        result = self.get_filtered_videos_from_url(url)\n",
    "        if not result['success']:\n",
    "            print(f\"第 {page} 页获取失败: {result['message']}\")\n",
    "            return [], False\n",
    "\n",
    "        filtered_vlist = result['filtered_videos']\n",
    "        all_videos_info = result['all_videos_info']\n",
    "\n",
    "        print(result['message'])\n",
    "\n",
    "        should_continue = False\n",
    "        if all_videos_info:\n",
    "            oldest_video_time = min(info['created_timestamp'] for info in all_videos_info)\n",
    "            newest_video_time = max(info['created_timestamp'] for info in all_videos_info)\n",
    "\n",
    "            print(f\"📅 当前页视频发布时间范围: {self.format_timestamp(oldest_video_time)} 到 {self.format_timestamp(newest_video_time)}\")\n",
    "\n",
    "            if newest_video_time < self.start_timestamp:\n",
    "                print(f\"⏹️  当前页面最新视频发布时间早于筛选开始时间，停止搜索后续页面。\")\n",
    "                should_continue = False\n",
    "            else:\n",
    "                should_continue = True\n",
    "\n",
    "        if not filtered_vlist and should_continue:\n",
    "            print(f'⚠️ 第 {page} 页没有符合时间范围的视频，但根据时间判断可能需要继续。')\n",
    "        elif not filtered_vlist and not should_continue:\n",
    "            print(f'⏹️ 第 {page} 页没有符合时间范围的视频，且已超出时间范围，停止搜索。')\n",
    "            return [], False\n",
    "\n",
    "        videos = []\n",
    "        print(f\"第 {page} 页开始处理 {len(filtered_vlist)} 个符合时间范围的视频\")\n",
    "\n",
    "        # 调整每个视频处理后的延时\n",
    "        DEFAULT_VIDEO_PROCESS_SLEEP = 2.5  # 从3秒减少到2秒\n",
    "\n",
    "        for i, video_data in enumerate(filtered_vlist):\n",
    "            title = video_data.get('title', 'N/A')\n",
    "            aid = video_data.get('aid')\n",
    "            description = video_data.get('description', 'N/A')\n",
    "            bvid = video_data.get('bvid', 'N/A')\n",
    "            video_url = f\"https://www.bilibili.com/video/{bvid}\"\n",
    "            length = video_data.get('length', 'N/A')\n",
    "            play = video_data.get('play', 0)\n",
    "            comment = video_data.get('comment', 0)\n",
    "            review = video_data.get('video_review', 0)\n",
    "            created = self.timestamp_to_datetime(video_data.get('created', 0))\n",
    "            pic_url = video_data.get('pic', 'N/A')\n",
    "            is_union_video_val = video_data.get('is_union_video', 0)\n",
    "            is_union_video = '是' if is_union_video_val == 1 else '否'\n",
    "\n",
    "            detail_info = self.get_video_detail(aid)\n",
    "            if detail_info:\n",
    "                (like, collect, coin, share, region, mid, name, fans, sign, level_info, duration_seconds,\n",
    "                 pages_count, copyright, video_state, related_count, sex, vip_type, vip_status, nameplate) = detail_info\n",
    "            else:\n",
    "                (like, collect, coin, share, region, mid, name, fans, sign, level_info, duration_seconds,\n",
    "                 pages_count, copyright, video_state, related_count, sex, vip_type, vip_status, nameplate) = (\n",
    "                    'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A',\n",
    "                    'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A'\n",
    "                )\n",
    "\n",
    "            video_info = {\n",
    "                'title': title, 'description': description, 'bvid': bvid, 'video_url': video_url,\n",
    "                'length': length, 'duration_seconds': duration_seconds,\n",
    "                'play': play, 'comment': comment, 'like': like, 'review': review, 'collect': collect,\n",
    "                'coin': coin, 'share': share, 'region': region, 'created': created, 'mid': mid,\n",
    "                'name': name, 'fans': fans, 'sign': sign, 'level_info': level_info,\n",
    "                'pic_url': pic_url, 'is_union_video': is_union_video, 'pages_count': pages_count,\n",
    "                'copyright': copyright, 'video_state': video_state, 'related_count': related_count,\n",
    "                'sex': sex, 'vip_type': vip_type, 'vip_status': vip_status, 'nameplate': nameplate,\n",
    "            }\n",
    "\n",
    "            videos.append(video_info)\n",
    "            print(f\"✅ 处理视频 {i+1}/{len(filtered_vlist)}: {title} (发布于: {created})\")\n",
    "\n",
    "            time.sleep(DEFAULT_VIDEO_PROCESS_SLEEP)\n",
    "\n",
    "        return videos, should_continue\n",
    "\n",
    "    def save_to_excel(self, videos, filename):\n",
    "        wb = Workbook()\n",
    "        ws = wb.active\n",
    "        ws.title = \"视频列表\"\n",
    "\n",
    "        headers = ['视频标题', '视频描述', '视频BV号', '视频URL', '封面URL', '视频时长', '视频时长秒', '是否合作视频', '多P数量',\n",
    "                   '版权', '视频状态', '相关视频数量', '播放', '评论', '点赞', '弹幕', '收藏', '投币', '分享',\n",
    "                   '所属分区', '发布时间','UID','昵称', '性别', '粉丝数','签名','是否认证/等级', '会员类型', '会员状态', '粉丝勋章']\n",
    "        ws.append(headers)\n",
    "\n",
    "        for video in videos:\n",
    "            row = [\n",
    "                video.get('title'), video.get('description'), video.get('bvid'), video.get('video_url'),\n",
    "                video.get('pic_url'), video.get('length'), video.get('duration_seconds'), video.get('is_union_video'),\n",
    "                video.get('pages_count'), video.get('copyright'), video.get('video_state'), video.get('related_count'),\n",
    "                video.get('play'), video.get('comment'), video.get('like'), video.get('review'), video.get('collect'),\n",
    "                video.get('coin'), video.get('share'), video.get('region'), video.get('created'), video.get('mid'),\n",
    "                video.get('name'), video.get('sex'), video.get('fans'), video.get('sign'), video.get('level_info'),\n",
    "                video.get('vip_type'), video.get('vip_status'), video.get('nameplate')\n",
    "            ]\n",
    "            ws.append(row)\n",
    "\n",
    "        wb.save(filename)\n",
    "        print(f\"💾 数据已保存到 {filename}\")\n",
    "\n",
    "    # --- 单个UP主完整处理 ---\n",
    "    def process_single_up_complete(self, mid, max_pages=50):  # 移除include_ai_summary参数\n",
    "        try:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"🎯 开始处理UP主: {mid}\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "            self.wbi_keys = None  # 重置WBI密钥\n",
    "\n",
    "            up_info = self.get_up_info(mid)\n",
    "            if not up_info:\n",
    "                up_name = f'UP主{mid}'\n",
    "                up_info = {'name': up_name, 'follower': 0, 'video': 0}\n",
    "            else:\n",
    "                up_name = up_info.get('name', f'UP主{mid}')\n",
    "\n",
    "            print(f\"👤 UP主昵称: {up_name}\")\n",
    "            print(f\"👥 粉丝数: {up_info.get('follower', 0):,}\")\n",
    "            print(f\"📺 投稿视频数: {up_info.get('video', 0):,}\")\n",
    "\n",
    "            print(f\"\\n📋 生成API URL列表...\")\n",
    "            self.api_urls = self.generate_api_urls(mid, max_pages)\n",
    "\n",
    "            if not self.api_urls:\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'up_name': up_name,\n",
    "                    'up_mid': mid,\n",
    "                    'video_count': 0,\n",
    "                    'error': '无法生成API URLs（可能WBI密钥获取失败或UP主无视频）'\n",
    "                }\n",
    "\n",
    "            print(f\"✅ 成功生成 {len(self.api_urls)} 个API URL\")\n",
    "\n",
    "            print(f\"\\n🚀 开始爬取指定时间范围内的视频数据...\")\n",
    "            all_videos = []\n",
    "\n",
    "            # 调整这里的页面间延时\n",
    "            DEFAULT_PAGE_INTERVAL_SLEEP = 6  # 从8秒减少到5秒\n",
    "\n",
    "            for page in sorted(self.api_urls.keys()):\n",
    "                print(f\"\\n🔄 正在爬取第 {page} 页...\")\n",
    "                videos_on_page, should_continue = self.crawl_videos_from_page_with_time_filter(page)\n",
    "\n",
    "                if videos_on_page:\n",
    "                    all_videos.extend(videos_on_page)\n",
    "                    print(f\"✅ 第 {page} 页完成，获取 {len(videos_on_page)} 个符合时间范围的视频\")\n",
    "                else:\n",
    "                    print(f\"⚠️ 第 {page} 页没有符合时间范围的视频。\")\n",
    "\n",
    "                if not should_continue:\n",
    "                    print(f\"🛑 已超出时间范围或所有视频均已处理，停止搜索后续页面。共搜索了 {page} 页\")\n",
    "                    break\n",
    "\n",
    "                if page < max(self.api_urls.keys()):\n",
    "                    print(f\"⏰ 页面间延时 {DEFAULT_PAGE_INTERVAL_SLEEP} 秒...\")\n",
    "                    time.sleep(DEFAULT_PAGE_INTERVAL_SLEEP)\n",
    "\n",
    "            if not all_videos:\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'up_name': up_name,\n",
    "                    'up_mid': mid,\n",
    "                    'video_count': 0,\n",
    "                    'error': '没有找到符合时间范围的视频，或视频列表API获取失败'\n",
    "                }\n",
    "\n",
    "            safe_up_name = sanitize_filename(up_name)  # 使用全局函数\n",
    "            time_range_str = f\"{self.start_date.replace('-', '')}_to_{self.end_date.replace('-', '')}\"\n",
    "            video_filename = f\"{safe_up_name}_{mid}_{time_range_str}_视频列表.xlsx\"\n",
    "            self.save_to_excel(all_videos, video_filename)\n",
    "\n",
    "            print(f\"\\n🎉 视频爬取完成！\")\n",
    "            print(f\"📊 时间范围内的视频总数: {len(all_videos)} 个\")\n",
    "            print(f\"📁 视频文件保存为: {video_filename}\")\n",
    "\n",
    "            return {\n",
    "                'success': True,\n",
    "                'up_name': up_name,\n",
    "                'up_mid': mid,\n",
    "                'video_count': len(all_videos),\n",
    "                'video_filename': video_filename,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"处理UP主{mid}时发生异常: {str(e)}\"\n",
    "            print(f\"❌ {error_msg}\")\n",
    "            print(traceback.format_exc())\n",
    "            return {\n",
    "                'success': False,\n",
    "                'up_name': f'UP主{mid}',\n",
    "                'up_mid': mid,\n",
    "                'video_count': 0,\n",
    "                'error': error_msg\n",
    "            }\n",
    "\n",
    "    # --- 批量处理主函数 ---\n",
    "    def batch_crawl_up_videos_complete(self, excel_path, start_index=0, end_index=None, max_pages_per_up=50):  # 移除include_ai_summary参数\n",
    "        print(\"=\"*100)\n",
    "        print(\"🚀 B站UP主批量完整爬取工具（视频信息版）\")\n",
    "        print(\"=\"*100)\n",
    "\n",
    "        # 检查Cookie池\n",
    "        if not self.cookie_strings_pool:\n",
    "            print(\"❌ Cookie池为空！请在config中提供有效的Cookie字符串。程序将退出。\")\n",
    "            return None\n",
    "        print(f\"✅ Cookie池已加载，共 {len(self.cookie_strings_pool)} 个Cookie。\")\n",
    "\n",
    "        print(\"🔍 测试Cookie有效性 (使用池中第一个Cookie)...\")\n",
    "        is_valid, result = self.test_cookie_validity()\n",
    "\n",
    "        if not is_valid:\n",
    "            print(f\"❌ 第一个Cookie无效或无法连接到B站API: {result}\")\n",
    "            print(\"请检查您的网络连接或更新 Cookie 字符串。程序将退出。\")\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"✅ Cookie有效！当前登录用户: {result['username']} (UID: {result['uid']})\")\n",
    "\n",
    "\n",
    "        up_mids = read_up_list_from_excel(excel_path, start_index, end_index)  # 使用全局函数\n",
    "        if not up_mids:\n",
    "            print(\"❌ 没有有效的UP主ID可处理\")\n",
    "            return None\n",
    "\n",
    "        print(f\"📅 时间筛选范围: {self.start_date} 到 {self.end_date}\")\n",
    "        print(f\"📄 每个UP主最大爬取页数: {max_pages_per_up}\")\n",
    "\n",
    "        total_ups = len(up_mids)\n",
    "        overall_results = []\n",
    "        failed_ups = []\n",
    "\n",
    "        print(f\"\\n🎯 开始批量处理 {total_ups} 个UP主...\")\n",
    "\n",
    "        # 调整这里的UP主间延时\n",
    "        DEFAULT_UP_INTERVAL_SLEEP = 10  # 从15秒减少到10秒\n",
    "\n",
    "        for i, mid in enumerate(up_mids, 1):\n",
    "            print(f\"\\n{'🔥' * 20}\")\n",
    "            print(f\"进度: [{i}/{total_ups}] 处理UP主: {mid}\")\n",
    "            print(f\"{'🔥' * 20}\")\n",
    "\n",
    "            try:\n",
    "                result = self.process_single_up_complete(mid, max_pages_per_up)  # 移除include_ai_summary参数\n",
    "                overall_results.append(result)\n",
    "\n",
    "                if result['success']:\n",
    "                    self.batch_stats['success_count'] += 1\n",
    "                    self.batch_stats['total_videos'] += result['video_count']\n",
    "                    print(f\"✅ 成功! 获取 {result['video_count']} 个视频\")\n",
    "                else:\n",
    "                    self.batch_stats['failed_count'] += 1\n",
    "                    failed_ups.append({\n",
    "                        'mid': mid,\n",
    "                        'error': result.get('error', '未知错误')\n",
    "                    })\n",
    "                    print(f\"❌ 失败: {result.get('error', '未知错误')}\")\n",
    "\n",
    "                self.batch_stats['total_processed'] += 1\n",
    "\n",
    "                print(f\"📊 当前统计: 成功 {self.batch_stats['success_count']}/{i}, \"\n",
    "                      f\"失败 {self.batch_stats['failed_count']}, \"\n",
    "                      f\"总视频 {self.batch_stats['total_videos']}\")\n",
    "\n",
    "                if i < total_ups:\n",
    "                    print(f\"⏰ UP主间延时 {DEFAULT_UP_INTERVAL_SLEEP} 秒...\")\n",
    "                    time.sleep(DEFAULT_UP_INTERVAL_SLEEP)\n",
    "\n",
    "            except Exception as e:\n",
    "                error_msg = f\"处理UP主{mid}时发生严重错误: {str(e)}\"\n",
    "                print(f\"❌ {error_msg}\")\n",
    "                self.batch_stats['failed_count'] += 1\n",
    "                failed_ups.append({\n",
    "                    'mid': mid,\n",
    "                    'error': error_msg\n",
    "                })\n",
    "                overall_results.append({\n",
    "                    'success': False,\n",
    "                    'up_name': f'UP主{mid}',\n",
    "                    'up_mid': mid,\n",
    "                    'video_count': 0,\n",
    "                    'error': error_msg\n",
    "                })\n",
    "\n",
    "        self.generate_batch_report_complete(overall_results, failed_ups)\n",
    "\n",
    "        return {\n",
    "            'total_processed': self.batch_stats['total_processed'],\n",
    "            'success_count': self.batch_stats['success_count'],\n",
    "            'failed_count': self.batch_stats['failed_count'],\n",
    "            'total_videos': self.batch_stats['total_videos'],\n",
    "            'results': overall_results,\n",
    "            'failed_ups': failed_ups\n",
    "        }\n",
    "\n",
    "    def generate_batch_report_complete(self, results, failed_ups):\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"📊 批量处理完成报告（视频信息版）\")\n",
    "        print(\"=\"*100)\n",
    "\n",
    "        print(f\"✅ 处理完成: {self.batch_stats['total_processed']} 个UP主\")\n",
    "        print(f\"🎉 成功: {self.batch_stats['success_count']} 个\")\n",
    "        print(f\"❌ 失败: {self.batch_stats['failed_count']} 个\")\n",
    "        print(f\"📺 总视频数: {self.batch_stats['total_videos']} 个\")\n",
    "\n",
    "        if self.batch_stats['success_count'] > 0:\n",
    "            print(f\"\\n✅ 成功处理的UP主:\")\n",
    "            for result in results:\n",
    "                if result['success']:\n",
    "                    print(f\"  - {result['up_name']} ({result.get('up_mid', 'N/A')}): \"\n",
    "                          f\"{result['video_count']} 个视频，文件: {result['video_filename']}\")\n",
    "\n",
    "        if failed_ups:\n",
    "            print(f\"\\n❌ 失败的UP主:\")\n",
    "            for failed in failed_ups:\n",
    "                print(f\"  - {failed['mid']}: {failed['error']}\")\n",
    "\n",
    "        report_filename = f\"批量处理报告_视频信息版_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "        try:\n",
    "            with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"B站UP主批量视频信息爬取处理报告\\n\")\n",
    "                f.write(\"=\"*50 + \"\\n\")\n",
    "                f.write(f\"处理时间: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "                f.write(f\"时间范围: {self.start_date} 到 {self.end_date}\\n\")\n",
    "                f.write(f\"总处理数: {self.batch_stats['total_processed']}\\n\")\n",
    "                f.write(f\"成功数: {self.batch_stats['success_count']}\\n\")\n",
    "                f.write(f\"失败数: {self.batch_stats['failed_count']}\\n\")\n",
    "                f.write(f\"总视频数: {self.batch_stats['total_videos']}\\n\\n\")\n",
    "\n",
    "                if self.batch_stats['success_count'] > 0:\n",
    "                    f.write(\"成功处理的UP主:\\n\")\n",
    "                    for result in results:\n",
    "                        if result['success']:\n",
    "                            f.write(f\"  - {result['up_name']} ({result.get('up_mid', 'N/A')}): \"\n",
    "                                    f\"{result['video_count']} 个视频，文件: {result['video_filename']}\\n\")\n",
    "\n",
    "                if failed_ups:\n",
    "                    f.write(\"\\n失败的UP主:\\n\")\n",
    "                    for failed in failed_ups:\n",
    "                        f.write(f\"  - {failed['mid']}: {failed['error']}\\n\")\n",
    "\n",
    "            print(f\"📄 处理报告已保存到: {report_filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 保存报告文件失败: {str(e)}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    print(\"=\"*100)\n",
    "    print(\"🎯 B站UP主视频信息批量爬取工具\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    # 检查配置\n",
    "    if not CONFIG['cookie_strings_pool'] or not any(c for c in CONFIG['cookie_strings_pool']):\n",
    "        print(\"❌ 请在CONFIG字典中设置至少一个有效的Cookie字符串到 'cookie_strings_pool'!\")\n",
    "        return\n",
    "    if not CONFIG['excel_path']:\n",
    "        print(\"❌ 请在CONFIG字典中设置你的excel文件路径!\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- 配置信息 ---\")\n",
    "    for key, value in CONFIG.items():\n",
    "        if key == 'cookie_strings_pool':\n",
    "            print(f\"  {key}: 已配置 {len(value)} 个Cookie\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    print(\"------------------\\n\")\n",
    "\n",
    "    crawler = BilibiliCrawler(CONFIG)  # 使用新的类名\n",
    "\n",
    "    # 重要的Cookie有效性测试\n",
    "    print(\"\\n🚀 正在测试Cookie有效性 (使用池中第一个Cookie)...\")\n",
    "    is_valid_cookie, cookie_check_info = crawler.test_cookie_validity()\n",
    "    \n",
    "    if not is_valid_cookie:\n",
    "        print(f\"❌ 第一个Cookie无效或无法连接到B站API: {cookie_check_info}\")\n",
    "        print(\"请检查您的网络连接或更新 'cookie_strings_pool' 中的Cookie字符串。程序将退出。\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"✅ Cookie有效！当前登录用户: {cookie_check_info.get('username', '未知')} (UID: {cookie_check_info.get('uid', '未知')})\")\n",
    "\n",
    "\n",
    "    print(f\"\\n📊 正在读取UP主MID列表从 {CONFIG['excel_path']} (索引范围: {CONFIG['start_index']} 到 {CONFIG['end_index'] if CONFIG['end_index'] is not None else '末尾'})...\")\n",
    "    \n",
    "    result_stats = crawler.batch_crawl_up_videos_complete(\n",
    "        excel_path=CONFIG['excel_path'],\n",
    "        start_index=CONFIG['start_index'],\n",
    "        end_index=CONFIG['end_index'],\n",
    "        max_pages_per_up=CONFIG['max_pages_per_up']\n",
    "    )\n",
    "\n",
    "    if result_stats:\n",
    "        print(f\"\\n🎉 批量处理完成！\")\n",
    "        print(f\"📊 最终统计:\")\n",
    "        print(f\"  总计处理UP主: {result_stats['total_processed']} 个\")\n",
    "        print(f\"  成功处理UP主: {result_stats['success_count']} 个\")\n",
    "        print(f\"  失败处理UP主: {result_stats['failed_count']} 个\")\n",
    "        print(f\"  获取视频总数: {result_stats['total_videos']} 个\")\n",
    "        if result_stats['failed_ups']:\n",
    "            print(\"\\n  以下UP主处理失败，请查看报告获取详情:\")\n",
    "            for f_up in result_stats['failed_ups']:\n",
    "                print(f\"    - MID: {f_up['mid']}, 错误: {f_up['error']}\")\n",
    "    else:\n",
    "        print(\"\\n❌ 批量处理失败或被中止！\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "🎯 B站UP主视频信息批量爬取工具\n",
      "====================================================================================================\n",
      "\n",
      "--- 配置信息 ---\n",
      "  excel_path: F:\\Code\\爬虫\\UP主ID.xlsx\n",
      "  cookie_strings_pool: 已配置 1 个Cookie\n",
      "  cookie_rotate_interval_seconds: 900\n",
      "  start_date: 2023-07-01\n",
      "  end_date: 2024-01-31\n",
      "  start_index: 750\n",
      "  end_index: None\n",
      "  max_pages_per_up: 20\n",
      "------------------\n",
      "\n",
      "✅ Cookie已切换至池中索引 0 的Cookie。\n",
      "\n",
      "🚀 正在测试Cookie有效性 (使用池中第一个Cookie)...\n",
      "✅ Cookie有效！当前登录用户: 测试用账号114514 (UID: 3546919328549020)\n",
      "\n",
      "📊 正在读取UP主MID列表从 F:\\Code\\爬虫\\UP主ID.xlsx (索引范围: 750 到 末尾)...\n",
      "====================================================================================================\n",
      "🚀 B站UP主批量完整爬取工具（视频信息版）\n",
      "====================================================================================================\n",
      "✅ Cookie池已加载，共 1 个Cookie。\n",
      "🔍 测试Cookie有效性 (使用池中第一个Cookie)...\n",
      "✅ Cookie有效！当前登录用户: 测试用账号114514 (UID: 3546919328549020)\n",
      "📊 正在读取Excel文件: F:\\Code\\爬虫\\UP主ID.xlsx\n",
      "✅ 成功读取到 2067 个UP主ID。\n",
      "🎯 选择处理范围: [750:2067]，共 1317 个UP主。\n",
      "📅 时间筛选范围: 2023-07-01 到 2024-01-31\n",
      "📄 每个UP主最大爬取页数: 20\n",
      "\n",
      "🎯 开始批量处理 1317 个UP主...\n",
      "\n",
      "🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥\n",
      "进度: [1/1317] 处理UP主: 3493088737626697\n",
      "🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥\n",
      "\n",
      "============================================================\n",
      "🎯 开始处理UP主: 3493088737626697\n",
      "============================================================\n",
      "详细信息获取失败: 访问权限不足 (Code: -403)\n",
      "🔄 尝试获取基础UP主信息...\n",
      "基础信息获取失败: 请求过于频繁，请稍后再试 (Code: -799)\n",
      "🔄 尝试从视频页面获取UP主信息...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1046\u001B[0m\n\u001B[0;32m   1042\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m❌ 批量处理失败或被中止！\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1045\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m-> 1046\u001B[0m     main()\n",
      "Cell \u001B[1;32mIn[1], line 1023\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m   1018\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m✅ Cookie有效！当前登录用户: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcookie_check_info\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124musername\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;250m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m未知\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (UID: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcookie_check_info\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muid\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;250m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m未知\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1021\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m📊 正在读取UP主MID列表从 \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCONFIG[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexcel_path\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (索引范围: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCONFIG[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstart_index\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m 到 \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCONFIG[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mend_index\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mif\u001B[39;00m\u001B[38;5;250m \u001B[39mCONFIG[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mend_index\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01mis\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01melse\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m末尾\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1023\u001B[0m result_stats \u001B[38;5;241m=\u001B[39m crawler\u001B[38;5;241m.\u001B[39mbatch_crawl_up_videos_complete(\n\u001B[0;32m   1024\u001B[0m     excel_path\u001B[38;5;241m=\u001B[39mCONFIG[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexcel_path\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m   1025\u001B[0m     start_index\u001B[38;5;241m=\u001B[39mCONFIG[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstart_index\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m   1026\u001B[0m     end_index\u001B[38;5;241m=\u001B[39mCONFIG[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mend_index\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m   1027\u001B[0m     max_pages_per_up\u001B[38;5;241m=\u001B[39mCONFIG[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_pages_per_up\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m   1028\u001B[0m )\n\u001B[0;32m   1030\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result_stats:\n\u001B[0;32m   1031\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m🎉 批量处理完成！\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[1], line 881\u001B[0m, in \u001B[0;36mBilibiliCrawler.batch_crawl_up_videos_complete\u001B[1;34m(self, excel_path, start_index, end_index, max_pages_per_up)\u001B[0m\n\u001B[0;32m    878\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m🔥\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m20\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    880\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 881\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_single_up_complete(mid, max_pages_per_up)  \u001B[38;5;66;03m# 移除include_ai_summary参数\u001B[39;00m\n\u001B[0;32m    882\u001B[0m     overall_results\u001B[38;5;241m.\u001B[39mappend(result)\n\u001B[0;32m    884\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msuccess\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n",
      "Cell \u001B[1;32mIn[1], line 748\u001B[0m, in \u001B[0;36mBilibiliCrawler.process_single_up_complete\u001B[1;34m(self, mid, max_pages)\u001B[0m\n\u001B[0;32m    744\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m60\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    746\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwbi_keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# 重置WBI密钥\u001B[39;00m\n\u001B[1;32m--> 748\u001B[0m up_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_up_info(mid)\n\u001B[0;32m    749\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m up_info:\n\u001B[0;32m    750\u001B[0m     up_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUP主\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmid\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n",
      "Cell \u001B[1;32mIn[1], line 390\u001B[0m, in \u001B[0;36mBilibiliCrawler.get_up_info\u001B[1;34m(self, mid)\u001B[0m\n\u001B[0;32m    388\u001B[0m first_url \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_api_url(mid, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    389\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m first_url:\n\u001B[1;32m--> 390\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_request(first_url)\n\u001B[0;32m    391\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m response \u001B[38;5;129;01mand\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m200\u001B[39m:\n\u001B[0;32m    392\u001B[0m         data \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mjson()\n",
      "Cell \u001B[1;32mIn[1], line 209\u001B[0m, in \u001B[0;36mBilibiliCrawler._make_request\u001B[1;34m(self, url, method, params, data, json_data, extra_headers)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m method\u001B[38;5;241m.\u001B[39mupper() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGET\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 209\u001B[0m         response \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mget(\n\u001B[0;32m    210\u001B[0m             url,\n\u001B[0;32m    211\u001B[0m             headers\u001B[38;5;241m=\u001B[39mrequest_headers,\n\u001B[0;32m    212\u001B[0m             cookies\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcookies,  \u001B[38;5;66;03m# 使用self.cookies对象，它会在_set_current_cookie中更新\u001B[39;00m\n\u001B[0;32m    213\u001B[0m             params\u001B[38;5;241m=\u001B[39mparams,\n\u001B[0;32m    214\u001B[0m             timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mDEFAULT_REQUEST_TIMEOUT  \u001B[38;5;66;03m# 使用类属性\u001B[39;00m\n\u001B[0;32m    215\u001B[0m         )\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m method\u001B[38;5;241m.\u001B[39mupper() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPOST\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    217\u001B[0m         response \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mpost(\n\u001B[0;32m    218\u001B[0m             url,\n\u001B[0;32m    219\u001B[0m             headers\u001B[38;5;241m=\u001B[39mrequest_headers,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    224\u001B[0m             timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mDEFAULT_REQUEST_TIMEOUT  \u001B[38;5;66;03m# 使用类属性\u001B[39;00m\n\u001B[0;32m    225\u001B[0m         )\n",
      "File \u001B[1;32mF:\\Anaconda\\Lib\\site-packages\\requests\\api.py:73\u001B[0m, in \u001B[0;36mget\u001B[1;34m(url, params, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(url, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     63\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request.\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \n\u001B[0;32m     65\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 73\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m request(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mget\u001B[39m\u001B[38;5;124m\"\u001B[39m, url, params\u001B[38;5;241m=\u001B[39mparams, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mF:\\Anaconda\\Lib\\site-packages\\requests\\api.py:59\u001B[0m, in \u001B[0;36mrequest\u001B[1;34m(method, url, **kwargs)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[1;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m session\u001B[38;5;241m.\u001B[39mrequest(method\u001B[38;5;241m=\u001B[39mmethod, url\u001B[38;5;241m=\u001B[39murl, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mF:\\Anaconda\\Lib\\site-packages\\requests\\sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[0;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[0;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[0;32m    587\u001B[0m }\n\u001B[0;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[1;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msend(prep, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39msend_kwargs)\n\u001B[0;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[1;32mF:\\Anaconda\\Lib\\site-packages\\requests\\sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[1;34m(self, request, **kwargs)\u001B[0m\n\u001B[0;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[0;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[1;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m adapter\u001B[38;5;241m.\u001B[39msend(request, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[0;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[1;32mF:\\Anaconda\\Lib\\site-packages\\requests\\adapters.py:667\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[0;32m    664\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m TimeoutSauce(connect\u001B[38;5;241m=\u001B[39mtimeout, read\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[0;32m    666\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 667\u001B[0m     resp \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39murlopen(\n\u001B[0;32m    668\u001B[0m         method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[0;32m    669\u001B[0m         url\u001B[38;5;241m=\u001B[39murl,\n\u001B[0;32m    670\u001B[0m         body\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mbody,\n\u001B[0;32m    671\u001B[0m         headers\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[0;32m    672\u001B[0m         redirect\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    673\u001B[0m         assert_same_host\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    674\u001B[0m         preload_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    675\u001B[0m         decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    676\u001B[0m         retries\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_retries,\n\u001B[0;32m    677\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[0;32m    678\u001B[0m         chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[0;32m    679\u001B[0m     )\n\u001B[0;32m    681\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m    682\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "File \u001B[1;32mF:\\Anaconda\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[0;32m    786\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    788\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[1;32m--> 789\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_request(\n\u001B[0;32m    790\u001B[0m     conn,\n\u001B[0;32m    791\u001B[0m     method,\n\u001B[0;32m    792\u001B[0m     url,\n\u001B[0;32m    793\u001B[0m     timeout\u001B[38;5;241m=\u001B[39mtimeout_obj,\n\u001B[0;32m    794\u001B[0m     body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[0;32m    795\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[0;32m    796\u001B[0m     chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[0;32m    797\u001B[0m     retries\u001B[38;5;241m=\u001B[39mretries,\n\u001B[0;32m    798\u001B[0m     response_conn\u001B[38;5;241m=\u001B[39mresponse_conn,\n\u001B[0;32m    799\u001B[0m     preload_content\u001B[38;5;241m=\u001B[39mpreload_content,\n\u001B[0;32m    800\u001B[0m     decode_content\u001B[38;5;241m=\u001B[39mdecode_content,\n\u001B[0;32m    801\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mresponse_kw,\n\u001B[0;32m    802\u001B[0m )\n\u001B[0;32m    804\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n\u001B[0;32m    805\u001B[0m clean_exit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32mF:\\Anaconda\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[0;32m    534\u001B[0m \u001B[38;5;66;03m# Receive the response from the server\u001B[39;00m\n\u001B[0;32m    535\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 536\u001B[0m     response \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39mgetresponse()\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (BaseSSLError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    538\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mread_timeout)\n",
      "File \u001B[1;32mF:\\Anaconda\\Lib\\site-packages\\urllib3\\connection.py:507\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    504\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mresponse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTTPResponse\n\u001B[0;32m    506\u001B[0m \u001B[38;5;66;03m# Get the response from http.client.HTTPConnection\u001B[39;00m\n\u001B[1;32m--> 507\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mgetresponse()\n\u001B[0;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    510\u001B[0m     assert_header_parsing(httplib_response\u001B[38;5;241m.\u001B[39mmsg)\n",
      "File \u001B[1;32mF:\\Anaconda\\Lib\\http\\client.py:1428\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1426\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1427\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1428\u001B[0m         response\u001B[38;5;241m.\u001B[39mbegin()\n\u001B[0;32m   1429\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n\u001B[0;32m   1430\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32mF:\\Anaconda\\Lib\\http\\client.py:331\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    329\u001B[0m \u001B[38;5;66;03m# read until we get a non-100 response\u001B[39;00m\n\u001B[0;32m    330\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 331\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_read_status()\n\u001B[0;32m    332\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n\u001B[0;32m    333\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32mF:\\Anaconda\\Lib\\http\\client.py:292\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    291\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 292\u001B[0m     line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp\u001B[38;5;241m.\u001B[39mreadline(_MAXLINE \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miso-8859-1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    293\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) \u001B[38;5;241m>\u001B[39m _MAXLINE:\n\u001B[0;32m    294\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m LineTooLong(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus line\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mF:\\Anaconda\\Lib\\socket.py:707\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    705\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 707\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[0;32m    708\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[0;32m    709\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32mF:\\Anaconda\\Lib\\ssl.py:1252\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[1;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[0;32m   1248\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1249\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1250\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[0;32m   1251\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[1;32m-> 1252\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread(nbytes, buffer)\n\u001B[0;32m   1253\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1254\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[1;32mF:\\Anaconda\\Lib\\ssl.py:1104\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[1;34m(self, len, buffer)\u001B[0m\n\u001B[0;32m   1102\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1103\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1104\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m, buffer)\n\u001B[0;32m   1105\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1106\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c41e935-8d4d-4cca-85f0-e208620fa49d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
